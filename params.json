{"name":"Machine Learning HTML Page by John Akwei, ECMp ERMp","tagline":"Coursera Practical Machine Learning Course Project","body":"## Machine Learning of Qualitative Human Activity  \r\n### John Akwei, ECMp ERMp  \r\n##### Wednesday, April 15, 2015  \r\n\r\n#### Synopsis:  \r\nThis document incorporates the activity monitoring device data from \"http://groupware.les.inf.puc-rio.br/har\", in order to predict the manner in which the test subjects performed a dumbbell lifting exercise. If the data from this document is used for any other research, please cite the preceding source because of generousity of data distribution.  \r\n\r\nThe data includes Euler angles, accelerometer, gyroscope, and magnetometer readings. The variable to predict is \"classe\", (involving 1 correct exercise and 4 types of exercise mistakes), and ranges from A to E.  \r\n\r\nFurther information on the data in this document is at: \"Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\"  \r\n\r\nThe training data source is at \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\". The test data source is at \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\".  \r\n\r\n#### Required Packages:  \r\nsetwd(\"C:/Users/johnakwei/Desktop/Coursera/PracticalMachineLearning/CourseProjectWriteUp\")  \r\nif (!require(\"plyr\")) { install.packages(\"plyr\"); require(\"plyr\") }  \r\nif (!require(\"caret\")) { install.packages(\"caret\"); require(\"caret\") }  \r\nif (!require(\"rpart\")) { install.packages(\"rpart\"); require(\"rpart\") }  \r\nif (!require(\"randomForest\")) { install.packages(\"randomForest\"); require(\"randomForest\") }  \r\nif (!require(\"e1071\")) { install.packages(\"e1071\"); require(\"e1071\") }  \r\n\r\n#### Data Input:  \r\ntraining <- read.csv(\"pml-training.csv\", na.strings=c(\"\",\"NA\",\"NULL\"))  \r\ntesting <- read.csv(\"pml-testing.csv\", na.strings=c(\"\",\"NA\",\"NULL\"))  \r\n\r\n#### Data Cleaning:  \r\nData with values of \"0\", and irrelevant categories, are removed from the dataset:  \r\ntraining <- training[, colSums(is.na(training))==0]  \r\ntesting <- testing[, colSums(is.na(testing))==0]  \r\ntraining <- training[, c(8:60)]  \r\ntesting <- testing[, c(8:60)]  \r\n\r\n#### Cross Validation:  \r\nThe dataset is divided into 70% training data and 30% testing data:  \r\ninTrain <- createDataPartition(y=training$classe, p=0.70, list=F)  \r\ntrain <- training[inTrain,]  \r\ntest <- training[-inTrain,]  \r\n\r\n#### Plot of Quantitive Data Gathering:  \r\nplot(train$classe, col=\"orange\", main=\"Plot of Quantity of Data by Data Class\", xlab=\"Data Classes\", ylab=\"Data Quantity\")  \r\n\r\n![Plot of Quantitive Data Gathering](https://www.dropbox.com/s/p8n1dzzbnavi4q3/PMLplot.png?dl=0)  \r\n\r\n#### Decision Tree Prediction Model:  \r\nmodel <- rpart(classe~., data=train, method=\"class\")  \r\nprediction  <- predict(model, test, type=\"class\")  \r\nconfusionMatrix(prediction, test$classe)  \r\n\r\nConfusion Matrix and Statistics  \r\n          Reference  \r\nPrediction    A    B    C    D    E  \r\n         A 1510  203   55  115   34  \r\n         B   52  615   56   64   79  \r\n         C   44   90  782  145  127  \r\n         D   44   91   69  517   47  \r\n         E   24  140   64  123  795  \r\n\r\nOverall Statistics:  \r\n               Accuracy : 0.7169  \r\n               95% CI : (0.7052, 0.7284)  \r\n    No Information Rate : 0.2845  \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n     \r\n                  Kappa : 0.6401  \r\n Mcnemar's Test P-Value : < 2.2e-16  \r\n\r\nStatistics by Class:  \r\n                     Class: A Class: B Class: C Class: D Class: E  \r\nSensitivity            0.9020   0.5399   0.7622  0.53631   0.7348  \r\nSpecificity            0.9033   0.9471   0.9164  0.94899   0.9269  \r\nPos Pred Value         0.7877   0.7102   0.6582  0.67318   0.6937  \r\nNeg Pred Value         0.9587   0.8956   0.9481  0.91264   0.9394  \r\nPrevalence             0.2845   0.1935   0.1743  0.16381   0.1839  \r\nDetection Rate         0.2566   0.1045   0.1329  0.08785   0.1351  \r\nDetection Prevalence   0.3257   0.1472   0.2019  0.13050   0.1947  \r\nBalanced Accuracy      0.9027   0.7435   0.8393  0.74265   0.8308  \r\n\r\n#### Random Forest Prediction Model:  \r\nmodel2 <- randomForest(classe~., data=train, na.action=na.omit)  \r\nprediction2 <- predict(model2, test, type=\"class\")  \r\nresult2 <- confusionMatrix(prediction2, test$classe)  \r\nresult2  \r\n\r\nConfusion Matrix and Statistics  \r\n          Reference  \r\nPrediction    A    B    C    D    E  \r\n         A 1674    6    0    0    0  \r\n         B    0 1132    3    0    0  \r\n         C    0    1 1022    5    0  \r\n         D    0    0    1  958    2  \r\n         E    0    0    0    1 1080  \r\n\r\nOverall Statistics:  \r\n               Accuracy : 0.9968  \r\n                 95% CI : (0.995, 0.9981)  \r\n    No Information Rate : 0.2845  \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n         \r\n                  Kappa : 0.9959  \r\n Mcnemar's Test P-Value : NA  \r\n\r\nStatistics by Class:  \r\n                     Class: A Class: B Class: C Class: D Class: E  \r\nSensitivity            1.0000   0.9939   0.9961   0.9938   0.9982  \r\nSpecificity            0.9986   0.9994   0.9988   0.9994   0.9998  \r\nPos Pred Value         0.9964   0.9974   0.9942   0.9969   0.9991  \r\nNeg Pred Value         1.0000   0.9985   0.9992   0.9988   0.9996  \r\nPrevalence             0.2845   0.1935   0.1743   0.1638   0.1839  \r\nDetection Rate         0.2845   0.1924   0.1737   0.1628   0.1835  \r\nDetection Prevalence   0.2855   0.1929   0.1747   0.1633   0.1837  \r\nBalanced Accuracy      0.9993   0.9966   0.9974   0.9966   0.9990  \r\n\r\n#### Expected Out-Of-Sample Error Rate:  \r\nOutErrorRate <- 1 - result2[[\"overall\"]][[\"Accuracy\"]]  \r\nOutErrorRate  \r\n\r\n[1] 0.003228547  \r\n\r\n#### Output files:  \r\nThe output files chosen were from the Random Forest algorithm:  \r\npredictionResult <- predict(model2, testing, type=\"class\")  \r\npredictionResult  \r\n\r\n1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  \r\nB  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B  \r\nLevels: A B C D E  \r\n\r\n#### Conclusion:  \r\nThe machine learning algorithm with the highest accuracy is the Random Forest model with 99.6% accuracy, and a 0.39% Out of Sample Error Rate. However, the level of over fitting with the Random Forest prediction model might not scale for other data.  \r\n\r\nThe inaccuracy of the predictions for Classes B through E, demonstrates the difficulty of predicting partial and inaccurate dumbbell movements by the sensory apparatus chosen for data gathering, as analyzed by Machine Learning algorithms.  ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}